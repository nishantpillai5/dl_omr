{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "EXf04EEa6dO6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.python.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Layer\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, LeakyReLU, Permute, Bidirectional\n",
        "from tensorflow.keras.layers import Reshape, Lambda, BatchNormalization, LSTM\n",
        "from tensorflow.python.keras.layers.merge import add, concatenate\n",
        "# from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "# from tensorflow.keras.layers import CuDNNLSTM\n",
        "from primus import CTC_PriMuS\n",
        "import ctc_utils\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip drive/MyDrive/Data/Data.zip"
      ],
      "metadata": {
        "id": "T7h2YraH60lA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PQSHimzQ6dO9"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-yeipd6m6dO-"
      },
      "outputs": [],
      "source": [
        "def default_model_params(img_height, vocabulary_size):\n",
        "    params = dict()\n",
        "    params['img_height'] = img_height\n",
        "    params['img_width'] = None\n",
        "    params['batch_size'] = 16\n",
        "    params['img_channels'] = 1\n",
        "    params['conv_blocks'] = 2\n",
        "    params['conv_filter_n'] = [32, 64]\n",
        "    params['conv_filter_size'] = [ [3,3], [3,3] ]\n",
        "    params['conv_pooling_size'] = [ [2,2], [2,2] ]\n",
        "    params['rnn_units'] = 32\n",
        "    params['rnn_layers'] = 1\n",
        "    params['vocabulary_size'] = vocabulary_size\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eEjfd5YW6dO-"
      },
      "outputs": [],
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") # 16\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        print(\"Y_pred shape:  \",y_pred.shape)\n",
        "        print(\"Y_true shape:  \",y_true.shape)\n",
        "\n",
        "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Q4Uj9uXi6dO_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ctc_crnn(params,width_rem = 128):\n",
        "    input_shape = (params['img_height'],params['img_width'], params['img_channels'])\n",
        "\n",
        "    inputs = Input(name='image', shape=input_shape, dtype='float32')\n",
        "\n",
        "    labels = Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
        "\n",
        "    width_reduction = 1\n",
        "    height_reduction = 1\n",
        "\n",
        "    #conv2d layer\n",
        "    for i in range(params['conv_blocks']):\n",
        "            inner = Conv2D(params['conv_filter_n'][i], params['conv_filter_size'][i], padding='same', name='conv'+ str(i+1), kernel_initializer='he_normal')(inputs if i == 0 else inner)\n",
        "            inner = BatchNormalization()(inner)\n",
        "            inner = LeakyReLU(0.2)(inner)\n",
        "            inner = MaxPooling2D(pool_size=params['conv_pooling_size'][i], strides = params['conv_pooling_size'][i], name='max' + str(i+1))(inner)\n",
        "\n",
        "            width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
        "            height_reduction = height_reduction * params['conv_pooling_size'][i][0]\n",
        "            print(width_reduction)\n",
        "    \n",
        "    print(width_reduction)\n",
        "    features = K.permute_dimensions(inner, (2,0,3,1))\n",
        "    feature_dim = params['conv_filter_n'][-1] * (params['img_height'] / height_reduction)\n",
        "    feature_width = width_rem / width_reduction\n",
        "    print(\"Feature width:\",feature_width)\n",
        "    features = tf.reshape(features, tf.stack([tf.cast(feature_width,'int32'), 16, tf.cast(feature_dim,'int32')]))\n",
        "    \n",
        "    # RNN block\n",
        "    lstm_1 = LSTM(params['rnn_units'], return_sequences=True, kernel_initializer='he_normal', name='lstm1')(features)  # (None, 32, 512)\n",
        "    lstm_1b = LSTM(params['rnn_units'], return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm1_b')(features)\n",
        "    reversed_lstm_1b = Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_1b)\n",
        "\n",
        "    lstm1_merged = add([lstm_1, reversed_lstm_1b])  # (None, 32, 512)\n",
        "    lstm1_merged = BatchNormalization()(lstm1_merged)\n",
        "    \n",
        "    lstm_2 = LSTM(params['rnn_units'], return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm1_merged)\n",
        "    lstm_2b = LSTM(params['rnn_units'], return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm2_b')(lstm1_merged)\n",
        "    reversed_lstm_2b= Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_2b)\n",
        "\n",
        "    lstm2_merged = concatenate([lstm_2, reversed_lstm_2b])  # (None, 32, 1024)\n",
        "    lstm2_merged = BatchNormalization()(lstm2_merged)\n",
        "\n",
        "    # transforms RNN output to character activations:\n",
        "    num_classes = params['vocabulary_size'] + 1\n",
        "    y_pred = Dense(num_classes, kernel_initializer='he_normal',name='dense2', activation='softmax')(lstm2_merged) #(None, 32, 63)\n",
        "    # y_pred = Activation('softmax', name='softmax')(inner)\n",
        "\n",
        "    # output = CTCLayer()(labels, y_pred)\n",
        "\n",
        "\n",
        "    return Model(inputs=[inputs, labels], outputs=y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "l3m-v5BJ6dPA",
        "outputId": "1796a3ac-a2e9-4d83-eed0-06c1b1331504",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 800 and validating with 200\n"
          ]
        }
      ],
      "source": [
        "corpus_dirpath = \"/content/primusCalvoRizoAppliedSciences2018/\"\n",
        "corpus_filepath = \"/content/train.txt\"\n",
        "dictionary_path = \"/content/vocabulary_semantic.txt\"\n",
        "primus = CTC_PriMuS(corpus_dirpath,corpus_filepath,dictionary_path, True, val_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NuwWK-_i6dPB"
      },
      "outputs": [],
      "source": [
        "# Get Data\n",
        "image_paths = []\n",
        "image_texts = []\n",
        "\n",
        "class PriMuS_Data:\n",
        "    def __init__(self, corpus_dirpath, corpus_filepath, dictionary_path):\n",
        "        self.corpus_dirpath = corpus_dirpath\n",
        "\n",
        "        # Corpus\n",
        "        corpus_file = open(corpus_filepath,'r')\n",
        "        self.corpus_list = corpus_file.read().splitlines()\n",
        "        corpus_file.close()\n",
        "\n",
        "        self.current_idx = 0\n",
        "\n",
        "        # Dictionary\n",
        "        self.word2int = {}\n",
        "        self.int2word = {}\n",
        "            \n",
        "        dict_file = open(dictionary_path,'r')\n",
        "        dict_list = dict_file.read().splitlines()\n",
        "        for word in dict_list:\n",
        "            if not word in self.word2int:\n",
        "                word_idx = len(self.word2int)\n",
        "                self.word2int[word] = word_idx\n",
        "                self.int2word[word_idx] = word\n",
        "\n",
        "        dict_file.close()\n",
        "\n",
        "        self.vocabulary_size = len(self.word2int)\n",
        "    \n",
        "    def get_vocabulary_size(self):\n",
        "        return self.vocabulary_size\n",
        "        \n",
        "    def get_data(self):\n",
        "        return self.corpus_list\n",
        "\n",
        "    def encode_seqs(self, seqs):\n",
        "        encoded = []\n",
        "        for seq in seqs:\n",
        "            new_seq=[]\n",
        "            for sym in seq:\n",
        "                new_seq.append(self.word2int[sym])\n",
        "            encoded.append(new_seq)\n",
        "        return encoded\n",
        "\n",
        "    def trim_ds(self):\n",
        "        val_split = 0.9\n",
        "        folder_path = self.corpus_dirpath\n",
        "        list_of_files = self.corpus_list #10,000 filenames\n",
        "\n",
        "        image_paths = [self.corpus_dirpath+f\"{x}/{x}.png\" for x in self.corpus_list] # list of image paths\n",
        "        text_paths = [self.corpus_dirpath+f\"{x}/{x}.semantic\" for x in self.corpus_list]\n",
        "\n",
        "        image_texts = [] # list of strings\n",
        "\n",
        "        for path in text_paths:\n",
        "            with open(path, \"r\") as file:\n",
        "                image_texts.append(file.readline().split())\n",
        "\n",
        "        image_texts = self.encode_seqs(image_texts)\n",
        "\n",
        "        max_label_len = max([len(seq) for seq in image_texts])\n",
        "        padded_image_texts = pad_sequences(image_texts, maxlen=max_label_len, padding='post', value= self.vocabulary_size + 1)\n",
        "\n",
        "        # TODO: do line 17 from the link : https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb\n",
        "\n",
        "        # TODO: Use self.word2int to encode each element of the lists in image_texts\n",
        "        # Input image_texts = [  ['clef','C-note'] , [.......], ....]\n",
        "        # Final image_texts = [  [5,3] , [.......], ....]\n",
        "        # if 'clef' == 5, 'C-note' == 3 in word2int dictionary\n",
        "        # Also add padding using the function below so length of each element is consistent\n",
        "\n",
        "        # from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "        # padded_image_texts = list(map(encode_to_labels, image_texts))\n",
        "        # padded_image_texts[0]\n",
        "\n",
        "\n",
        "        train_image_paths = image_paths[ : int(len(image_paths) * val_split)]\n",
        "        train_image_texts = padded_image_texts[ : int(len(padded_image_texts) * val_split)]\n",
        "\n",
        "        val_image_paths = image_paths[int(len(image_paths) * val_split) : ]\n",
        "        val_image_texts = padded_image_texts[int(len(padded_image_texts) * val_split) : ]\n",
        "\n",
        "        return {\"train\":{\"images\":train_image_paths, \"text\": train_image_texts}, \"val\":{\"images\":val_image_paths, \"text\": val_image_texts}}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9VGGvrsm6dPC"
      },
      "outputs": [],
      "source": [
        "data_obj = PriMuS_Data(corpus_dirpath, corpus_filepath, dictionary_path)\n",
        "lst = data_obj.trim_ds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "n5H0IDB86dPD"
      },
      "outputs": [],
      "source": [
        "# TODO: do line 17 from the link : https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "jIE1pbZO6dPD",
        "outputId": "936bcd74-3e30-4d76-e8b9-00af05b5cebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,  229, 1757,  236,    0,  687, 1280, 1303, 1303,  823,    0,\n",
              "        490,  490, 1727, 1727,    0, 1470, 1304,  853,  679,  501,    0,\n",
              "        490, 1599, 1727, 1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782,\n",
              "       1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782,\n",
              "       1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782, 1782,\n",
              "       1782], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "lst[\"train\"][\"text\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "l5x53FqU6dPE"
      },
      "outputs": [],
      "source": [
        "# Resize images to desired dimensions with unknown width\n",
        "def resize_no_width(image, height):\n",
        "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
        "    sample_img = cv2.resize(image, (width, height))\n",
        "    return sample_img\n",
        "\n",
        "\n",
        "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
        "def process_single_sample(img_path, label,max_width = 30):\n",
        "    params = default_model_params(img_height,primus.vocabulary_size)\n",
        "    # 1. Read image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img,[params['img_height'], max_width])\n",
        "    # TODO: Get height from params, calculate width from max of all images\n",
        "    return {\"image\": img, \"label\": label}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip drive/MyDrive/Data/Data.zip"
      ],
      "metadata": {
        "id": "dLvhNRYJ_-vN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3a1PjF526dPE",
        "outputId": "5a46140f-108b-48d2-a32d-221dafba9d0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2003\n",
            "2\n",
            "4\n",
            "4\n",
            "Feature width: 500.75\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " image (InputLayer)             [(None, 64, None, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)                 (None, 64, None, 32  320         ['image[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 64, None, 32  128        ['conv1[0][0]']                  \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_8 (LeakyReLU)      (None, 64, None, 32  0           ['batch_normalization_16[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max1 (MaxPooling2D)            (None, 32, None, 32  0           ['leaky_re_lu_8[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2 (Conv2D)                 (None, 32, None, 64  18496       ['max1[0][0]']                   \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 32, None, 64  256        ['conv2[0][0]']                  \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_9 (LeakyReLU)      (None, 32, None, 64  0           ['batch_normalization_17[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max2 (MaxPooling2D)            (None, 16, None, 64  0           ['leaky_re_lu_9[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_14 (TFO  (None, None, 64, 16  0          ['max2[0][0]']                   \n",
            " pLambda)                       )                                                                 \n",
            "                                                                                                  \n",
            " tf.reshape_4 (TFOpLambda)      (500, 16, 1024)      0           ['tf.compat.v1.transpose_14[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " lstm1_b (LSTM)                 (500, 16, 32)        135296      ['tf.reshape_4[0][0]']           \n",
            "                                                                                                  \n",
            " lstm1 (LSTM)                   (500, 16, 32)        135296      ['tf.reshape_4[0][0]']           \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (500, 16, 32)        0           ['lstm1_b[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (500, 16, 32)       0           ['lstm1[0][0]',                  \n",
            " mbda)                                                            'lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (500, 16, 32)       128         ['tf.__operators__.add_6[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " lstm2_b (LSTM)                 (500, 16, 32)        8320        ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " lstm2 (LSTM)                   (500, 16, 32)        8320        ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (500, 16, 32)        0           ['lstm2_b[0][0]']                \n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)       (500, 16, 64)        0           ['lstm2[0][0]',                  \n",
            "                                                                  'lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (500, 16, 64)       256         ['tf.concat_4[0][0]']            \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " label (InputLayer)             [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dense2 (Dense)                 (500, 16, 1782)      115830      ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 422,646\n",
            "Trainable params: 422,262\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "\n",
        "\n",
        "image_widths = [cv2.imread(img).shape[1] for img in lst[\"train\"][\"images\"]]\n",
        "max_image_width = max(image_widths)\n",
        "print(max_image_width)\n",
        "# Parameterization\n",
        "img_height = 64\n",
        "params = default_model_params(img_height,primus.vocabulary_size)\n",
        "max_epochs = 100\n",
        "dropout = 0.5\n",
        "# Model\n",
        "model = ctc_crnn(params, width_rem= max_image_width)\n",
        "model.summary()\n",
        "\n",
        "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
        "def process_single_sample(img_path, label):\n",
        "    params = default_model_params(img_height,primus.vocabulary_size)\n",
        "    # 1. Read image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img,[params['img_height'], max_image_width])\n",
        "    # TODO: Get height from params, calculate width from max of all images\n",
        "    return {\"image\": img, \"label\": label}\n",
        "# train_dataset = train_dataset.map(process_single_sample(max_width = max_image_width))\n",
        "\n",
        "# processed_images = [process_single_sample(img,label,max_image_width) for img,label in zip(lst[\"train\"][\"images\"], lst[\"train\"][\"text\"])]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.map(\n",
        "        process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(\n",
        "        process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "uzNrFgjj6dPF",
        "outputId": "90a5e12e-389c-4d42-beb0-2945c91d0bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.Series(image_widths)\n",
        "plt.hist(df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIuRdZzx6dPF"
      },
      "source": [
        "https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "dcX6JoDN6dPH",
        "outputId": "2c71a892-1006-485c-ab79-24bc086acf09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-f4abd18824b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;31m# callbacks = callbacks_list,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     shuffle=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 861, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 819, in _validate_target_and_loss\n        'Target data is missing. Your model was compiled with '\n\n    ValueError: Target data is missing. Your model was compiled with loss=<function CTCLoss at 0x7f227f4dd290>, and therefore expects target data to be provided in `fit()`.\n"
          ]
        }
      ],
      "source": [
        "# Compile\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
        "model.compile(optimizer= optimizer, loss = CTCLoss)\n",
        "\n",
        "# checkpoint = ModelCheckpoint(filepath=file_path, \n",
        "#                                 monitor='val_loss', \n",
        "#                                 verbose=1, \n",
        "#                                 save_best_only=True, \n",
        "#                                 mode='min')\n",
        "\n",
        "#     callbacks_list = [checkpoint, \n",
        "#                       WandbCallback(monitor=\"val_loss\", \n",
        "#                                     mode=\"min\", \n",
        "#                                     log_weights=True),\n",
        "#                       PlotPredictions(frequency=1),\n",
        "#                       EarlyStopping(patience=3, verbose=1)]\n",
        "\n",
        "history = model.fit(train_dataset, \n",
        "                    epochs = max_epochs,\n",
        "                    validation_data=validation_dataset,\n",
        "                    verbose = 1,\n",
        "                    # callbacks = callbacks_list,\n",
        "                    shuffle=True)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "e9bbe03f6c4bbdf56f443191d16980388af1b731aba7be9951b0256cee325895"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('tf-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ctc_model_fit.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}