{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://keras.io/examples/audio/ctc_asr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Layer\n",
    "# from tensorflow.keras.layers import Input, Dense, Activation, LeakyReLU, Permute, Bidirectional\n",
    "# from tensorflow.keras.layers import Reshape, Lambda, BatchNormalization, LSTM\n",
    "# from tensorflow.python.keras.layers.merge import add, concatenate\n",
    "# from tensorflow.python.keras.layers.recurrent import LSTM\n",
    "# from tensorflow.keras.layers import CuDNNLSTM\n",
    "from primus import CTC_PriMuS\n",
    "import ctc_utils\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_model_params(img_height, vocabulary_size):\n",
    "    params = dict()\n",
    "    params['img_height'] = img_height\n",
    "    params['img_width'] = None\n",
    "    params['batch_size'] = 16\n",
    "    params['img_channels'] = 1\n",
    "    params['conv_blocks'] = 2\n",
    "    params['conv_filter_n'] = [32, 64]\n",
    "    params['conv_filter_size'] = [ [3,3], [3,3] ]\n",
    "    params['conv_pooling_size'] = [ [2,2], [2,2] ]\n",
    "    params['rnn_units'] = 32\n",
    "    params['rnn_layers'] = 1\n",
    "    params['vocabulary_size'] = vocabulary_size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") # 16\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        print(\"Y_pred shape:  \",y_pred.shape)\n",
    "        print(\"Y_true shape:  \",y_true.shape)\n",
    "\n",
    "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def buildModel(params, input_width, rnn_layers = 1, rnn_units = 64):\n",
    "    input_shape = (params['img_height'],params['img_width'], params['img_channels'])\n",
    "\n",
    "    \n",
    "    inputs = layers.Input(type_spec=tf.TensorSpec(shape=[None, params['img_height'], None, 1], dtype=tf.float32), name = \"image\" )\n",
    "    # inputs = layers.Input(name='image', shape=input_shape, dtype='float32')\n",
    "\n",
    "    labels = layers.Input(type_spec=tf.TensorSpec(shape=[None, None], dtype=tf.float32), name = \"label\" )\n",
    "    # x = layers.Reshape((params['img_height'], input_width, 1), name=\"expand_dim\")(inputs)\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size= [params['conv_filter_n'][0], params['conv_filter_n'][0]],\n",
    "        strides=[2, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_1\",\n",
    "    )(inputs)\n",
    "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
    "    x = layers.MaxPool2D(pool_size=params['conv_pooling_size'][0], strides = params['conv_pooling_size'][0], name='max1')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=[params['conv_filter_n'][1], params['conv_filter_n'][1]],\n",
    "        strides=[1, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_2\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
    "    x = layers.MaxPool2D(pool_size=params['conv_pooling_size'][1], strides = params['conv_pooling_size'][1], name='max2')(x)\n",
    "    print(x.shape)\n",
    "    x = layers.Reshape((-1, x.shape[-3] * x.shape[-1]))(x)\n",
    "\n",
    "    for i in range(1, rnn_layers + 1):\n",
    "        recurrent = layers.GRU(\n",
    "            units=rnn_units,\n",
    "            activation=\"tanh\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            reset_after=True,\n",
    "            name=f\"gru_{i}\",\n",
    "        )\n",
    "        x = layers.Bidirectional(\n",
    "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
    "        )(x)\n",
    "        if i < rnn_layers:\n",
    "            x = layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "    num_classes = params['vocabulary_size'] + 1\n",
    "    y_pred = layers.Dense(num_classes, kernel_initializer='he_normal',name='dense2', activation='softmax')(x) #(None, 32, 63)        \n",
    "\n",
    "    model = Model([inputs, labels], y_pred, name=\"OMR\")\n",
    "    opt = Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=opt, loss=CTCLoss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 800 and validating with 200\n"
     ]
    }
   ],
   "source": [
    "img_height = 64\n",
    "corpus_dirpath = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/primusCalvoRizoAppliedSciences2018/\"\n",
    "corpus_filepath = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/train.txt\"\n",
    "dictionary_path = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/vocabulary_semantic.txt\"\n",
    "primus = CTC_PriMuS(corpus_dirpath,corpus_filepath,dictionary_path, True, val_split = 0.2)\n",
    "# corpus_dirpath = \"/content/primusCalvoRizoAppliedSciences2018/\"\n",
    "# corpus_filepath = \"/content/train.txt\"\n",
    "# dictionary_path = \"/content/vocabulary_semantic.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "image_paths = []\n",
    "image_texts = []\n",
    "\n",
    "class PriMuS_Data:\n",
    "    def __init__(self, corpus_dirpath, corpus_filepath, dictionary_path):\n",
    "        self.corpus_dirpath = corpus_dirpath\n",
    "\n",
    "        # Corpus\n",
    "        corpus_file = open(corpus_filepath,'r')\n",
    "        self.corpus_list = corpus_file.read().splitlines()\n",
    "        corpus_file.close()\n",
    "\n",
    "        self.current_idx = 0\n",
    "\n",
    "        # Dictionary\n",
    "        self.word2int = {}\n",
    "        self.int2word = {}\n",
    "            \n",
    "        dict_file = open(dictionary_path,'r')\n",
    "        dict_list = dict_file.read().splitlines()\n",
    "        for word in dict_list:\n",
    "            if not word in self.word2int:\n",
    "                word_idx = len(self.word2int)\n",
    "                self.word2int[word] = word_idx\n",
    "                self.int2word[word_idx] = word\n",
    "\n",
    "        dict_file.close()\n",
    "\n",
    "        self.vocabulary_size = len(self.word2int)\n",
    "    \n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vocabulary_size\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.corpus_list\n",
    "\n",
    "    def encode_seqs(self, seqs):\n",
    "        encoded = []\n",
    "        for seq in seqs:\n",
    "            new_seq=[]\n",
    "            for sym in seq:\n",
    "                new_seq.append(self.word2int[sym])\n",
    "            encoded.append(new_seq)\n",
    "        return encoded\n",
    "\n",
    "    def trim_ds(self):\n",
    "        val_split = 0.9\n",
    "        folder_path = self.corpus_dirpath\n",
    "        list_of_files = self.corpus_list #10,000 filenames\n",
    "\n",
    "        image_paths = [self.corpus_dirpath+f\"{x}/{x}.png\" for x in self.corpus_list] # list of image paths\n",
    "        text_paths = [self.corpus_dirpath+f\"{x}/{x}.semantic\" for x in self.corpus_list]\n",
    "\n",
    "        image_texts = [] # list of strings\n",
    "\n",
    "        for path in text_paths:\n",
    "            with open(path, \"r\") as file:\n",
    "                image_texts.append(file.readline().split())\n",
    "\n",
    "        image_texts = self.encode_seqs(image_texts)\n",
    "\n",
    "        max_label_len = max([len(seq) for seq in image_texts])\n",
    "        padded_image_texts = pad_sequences(image_texts, maxlen=max_label_len, padding='post', value= self.vocabulary_size + 1)\n",
    "\n",
    "        # TODO: do line 17 from the link : https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb\n",
    "\n",
    "        # TODO: Use self.word2int to encode each element of the lists in image_texts\n",
    "        # Input image_texts = [  ['clef','C-note'] , [.......], ....]\n",
    "        # Final image_texts = [  [5,3] , [.......], ....]\n",
    "        # if 'clef' == 5, 'C-note' == 3 in word2int dictionary\n",
    "        # Also add padding using the function below so length of each element is consistent\n",
    "\n",
    "        # from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        # padded_image_texts = list(map(encode_to_labels, image_texts))\n",
    "        # padded_image_texts[0]\n",
    "\n",
    "\n",
    "        train_image_paths = image_paths[ : int(len(image_paths) * val_split)]\n",
    "        train_image_texts = padded_image_texts[ : int(len(padded_image_texts) * val_split)]\n",
    "\n",
    "        val_image_paths = image_paths[int(len(image_paths) * val_split) : ]\n",
    "        val_image_texts = padded_image_texts[int(len(padded_image_texts) * val_split) : ]\n",
    "\n",
    "        return {\"train\":{\"images\":train_image_paths, \"text\": train_image_texts}, \"val\":{\"images\":val_image_paths, \"text\": val_image_texts}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = PriMuS_Data(corpus_dirpath, corpus_filepath, dictionary_path)\n",
    "lst = data_obj.trim_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to desired dimensions with unknown width\n",
    "def resize_no_width(image, height):\n",
    "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
    "    sample_img = cv2.resize(image, (width, height))\n",
    "    return sample_img\n",
    "\n",
    "\n",
    "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
    "def process_single_sample(img_path, label,max_width = 30):\n",
    "    params = default_model_params(img_height,primus.vocabulary_size)\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img,[params['img_height'], max_width])\n",
    "    # TODO: Get height from params, calculate width from max of all images\n",
    "    return {\"image\": img, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "(None, 8, None, 64)\n",
      "Model: \"OMR\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 64, None, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv_1 (Conv2D)                (None, 32, None, 32  32768       ['image[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_1_bn (BatchNormalization)  (None, 32, None, 32  128        ['conv_1[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_1_relu (ReLU)             (None, 32, None, 32  0           ['conv_1_bn[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max1 (MaxPooling2D)            (None, 16, None, 32  0           ['conv_1_relu[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_2 (Conv2D)                (None, 16, None, 64  8388608     ['max1[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_2_bn (BatchNormalization)  (None, 16, None, 64  256        ['conv_2[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_2_relu (ReLU)             (None, 16, None, 64  0           ['conv_2_bn[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max2 (MaxPooling2D)            (None, 8, None, 64)  0           ['conv_2_relu[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, None, 512)    0           ['max2[0][0]']                   \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, None, 128)   221952      ['reshape_2[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " label (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, None, 1782)   229878      ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,873,590\n",
      "Trainable params: 8,873,398\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "image_widths = [cv2.imread(img).shape[1] for img in lst[\"train\"][\"images\"]]\n",
    "max_image_width = max(image_widths)\n",
    "print(max_image_width)\n",
    "# Parameterization\n",
    "img_height = 64\n",
    "params = default_model_params(img_height,primus.vocabulary_size)\n",
    "max_epochs = 100\n",
    "dropout = 0.5\n",
    "# Model\n",
    "model = buildModel(params,input_width = max_image_width)\n",
    "model.summary()\n",
    "\n",
    "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
    "def process_single_sample(img_path, label):\n",
    "    params = default_model_params(img_height,primus.vocabulary_size)\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img,[params['img_height'], max_image_width])\n",
    "    # TODO: Get height from params, calculate width from max of all images\n",
    "    return {\"image\": img, \"label\": label}\n",
    "# train_dataset = train_dataset.map(process_single_sample(max_width = max_image_width))\n",
    "\n",
    "# processed_images = [process_single_sample(img,label,max_image_width) for img,label in zip(lst[\"train\"][\"images\"], lst[\"train\"][\"text\"])]\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
    "\n",
    "# train_dataset = (\n",
    "#     train_dataset.map(\n",
    "#         process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "#     )\n",
    "#     .batch(batch_size)\n",
    "#     .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "# )\n",
    "\n",
    "# validation_dataset = tf.data.Dataset.from_tensor_slices((lst['val']['images'], lst['val']['text']))\n",
    "# validation_dataset = (\n",
    "#     validation_dataset.map(\n",
    "#         process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "#     )\n",
    "#     .batch(batch_size)\n",
    "#     .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "# )\n",
    "def tfdata_generator_train(batch_size= 16):\n",
    "  '''Construct a data generator using `tf.Dataset`. '''\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
    "  train_dataset = train_dataset.map(process_single_sample)\n",
    "  train_dataset = train_dataset.batch(batch_size)\n",
    "  # train_dataset = train_dataset.repeat()\n",
    "  train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  return train_dataset\n",
    "\n",
    "def tfdata_generator_val(batch_size= 16):\n",
    "  '''Construct a data generator using `tf.Dataset`. '''\n",
    "  val_dataset = tf.data.Dataset.from_tensor_slices((lst['val']['images'], lst['val']['text']))\n",
    "  val_dataset = val_dataset.map(process_single_sample)\n",
    "  val_dataset = val_dataset.batch(batch_size)\n",
    "  # val_dataset = val_dataset.repeat()\n",
    "  val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  return val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tfdata_generator_train()\n",
    "validation_dataset = tfdata_generator_val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 861, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 818, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=<function CTCLoss at 0x000002D2D19084C0>, and therefore expects target data to be provided in `fit()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steli\\DeepLearning\\Project\\dl_omr\\notebooks\\ctc_model_fit_New_perspective.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=1'>2</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=5'>6</a>\u001b[0m     train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=6'>7</a>\u001b[0m     \u001b[39m# tf.compat.v1.data.make_one_shot_iterator(train_dataset).get_next(),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=7'>8</a>\u001b[0m     \u001b[39m# validation_data = tf.compat.v1.data.make_one_shot_iterator(validation_dataset).get_next(),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=8'>9</a>\u001b[0m     validation_data \u001b[39m=\u001b[39;49m validation_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=9'>10</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000013?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 861, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 818, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=<function CTCLoss at 0x000002D2D19084C0>, and therefore expects target data to be provided in `fit()`.\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs.\n",
    "epochs = 30\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    # tf.compat.v1.data.make_one_shot_iterator(train_dataset).get_next(),\n",
    "    # validation_data = tf.compat.v1.data.make_one_shot_iterator(validation_dataset).get_next(),\n",
    "    validation_data = validation_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9bbe03f6c4bbdf56f443191d16980388af1b731aba7be9951b0256cee325895"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
