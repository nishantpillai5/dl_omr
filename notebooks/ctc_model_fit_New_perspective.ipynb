{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://keras.io/examples/audio/ctc_asr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Layer\n",
    "# from tensorflow.keras.layers import Input, Dense, Activation, LeakyReLU, Permute, Bidirectional\n",
    "# from tensorflow.keras.layers import Reshape, Lambda, BatchNormalization, LSTM\n",
    "# from tensorflow.python.keras.layers.merge import add, concatenate\n",
    "# from tensorflow.python.keras.layers.recurrent import LSTM\n",
    "# from tensorflow.keras.layers import CuDNNLSTM\n",
    "from primus import CTC_PriMuS\n",
    "import ctc_utils\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_model_params(img_height, vocabulary_size):\n",
    "    params = dict()\n",
    "    params['img_height'] = img_height\n",
    "    params['img_width'] = None\n",
    "    params['batch_size'] = 16\n",
    "    params['img_channels'] = 1\n",
    "    params['conv_blocks'] = 2\n",
    "    params['conv_filter_n'] = [32, 64]\n",
    "    params['conv_filter_size'] = [ [3,3], [3,3] ]\n",
    "    params['conv_pooling_size'] = [ [2,2], [2,2] ]\n",
    "    params['rnn_units'] = 32\n",
    "    params['rnn_layers'] = 1\n",
    "    params['vocabulary_size'] = vocabulary_size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = K.ctc_batch_cost\n",
    "        # self.loss_fn = K.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") # 16\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        # print(\"Y_pred shape:  \",y_pred.shape)\n",
    "        # print(\"Y_true shape:  \",y_true.shape)\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def buildModel(params, input_width, rnn_layers = 2, rnn_units = 128):\n",
    "    input_shape = (params['img_height'],params['img_width'], params['img_channels'])\n",
    "\n",
    "    inputs = layers.Input(type_spec=tf.TensorSpec(shape=[None, params['img_height'], input_width, 1], dtype=tf.float32), name = \"image\" )\n",
    "    # inputs = layers.Input(name='image', shape=input_shape, dtype='float32')\n",
    "\n",
    "    labels = layers.Input(type_spec=tf.TensorSpec(shape=[None, None], dtype=tf.float32), name = \"label\" )\n",
    "    # x = layers.Reshape((params['img_height'], input_width, 1), name=\"expand_dim\")(inputs)\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size= [params['conv_filter_n'][0], params['conv_filter_n'][0]],\n",
    "        strides=[2, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_1\",\n",
    "    )(inputs)\n",
    "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
    "    x = layers.MaxPool2D(pool_size=params['conv_pooling_size'][0], strides = params['conv_pooling_size'][0], name='max1')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=[params['conv_filter_n'][1], params['conv_filter_n'][1]],\n",
    "        strides=[1, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_2\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
    "    x = layers.MaxPool2D(pool_size=params['conv_pooling_size'][1], strides = params['conv_pooling_size'][1], name='max2')(x)\n",
    "    print(x.shape)\n",
    "    x = layers.Reshape((-1, x.shape[-3] * x.shape[-1]))(x)\n",
    "\n",
    "    for i in range(1, rnn_layers + 1):\n",
    "        recurrent = layers.GRU(\n",
    "            units=rnn_units,\n",
    "            activation=\"tanh\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            reset_after=True,\n",
    "            name=f\"gru_{i}\",\n",
    "        )\n",
    "        x = layers.Bidirectional(\n",
    "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
    "        )(x)\n",
    "        if i < rnn_layers:\n",
    "            x = layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "    num_classes = params['vocabulary_size'] + 1\n",
    "    y_pred = layers.Dense(num_classes, kernel_initializer='he_normal',name='dense2', activation='softmax')(x) #(None, 32, 63)        \n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, y_pred)\n",
    "    model = Model([inputs,labels], output, name=\"OMR\")\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "    model.compile(optimizer = optimizer)\n",
    "    # model.compile(optimizer=opt, loss=CTCLoss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 800 and validating with 200\n"
     ]
    }
   ],
   "source": [
    "img_height = 64\n",
    "corpus_dirpath = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/primusCalvoRizoAppliedSciences2018/\"\n",
    "corpus_filepath = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/train.txt\"\n",
    "dictionary_path = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/vocabulary_semantic.txt\"\n",
    "primus = CTC_PriMuS(corpus_dirpath,corpus_filepath,dictionary_path, True, val_split = 0.2)\n",
    "# corpus_dirpath = \"/content/primusCalvoRizoAppliedSciences2018/\"\n",
    "# corpus_filepath = \"/content/train.txt\"\n",
    "# dictionary_path = \"/content/vocabulary_semantic.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "image_paths = []\n",
    "image_texts = []\n",
    "\n",
    "class PriMuS_Data:\n",
    "    def __init__(self, corpus_dirpath, corpus_filepath, dictionary_path):\n",
    "        self.corpus_dirpath = corpus_dirpath\n",
    "\n",
    "        # Corpus\n",
    "        corpus_file = open(corpus_filepath,'r')\n",
    "        self.corpus_list = corpus_file.read().splitlines()\n",
    "        corpus_file.close()\n",
    "\n",
    "        self.current_idx = 0\n",
    "\n",
    "        # Dictionary\n",
    "        self.word2int = {}\n",
    "        self.int2word = {}\n",
    "            \n",
    "        dict_file = open(dictionary_path,'r')\n",
    "        dict_list = dict_file.read().splitlines()\n",
    "        for word in dict_list:\n",
    "            if not word in self.word2int:\n",
    "                word_idx = len(self.word2int)\n",
    "                self.word2int[word] = word_idx\n",
    "                self.int2word[word_idx] = word\n",
    "\n",
    "        dict_file.close()\n",
    "\n",
    "        self.vocabulary_size = len(self.word2int)\n",
    "    \n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vocabulary_size\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.corpus_list\n",
    "\n",
    "    def encode_seqs(self, seqs):\n",
    "        encoded = []\n",
    "        for seq in seqs:\n",
    "            new_seq=[]\n",
    "            for sym in seq:\n",
    "                new_seq.append(self.word2int[sym])\n",
    "            encoded.append(new_seq)\n",
    "        return encoded\n",
    "\n",
    "    def trim_ds(self):\n",
    "        val_split = 0.9\n",
    "        folder_path = self.corpus_dirpath\n",
    "        list_of_files = self.corpus_list #10,000 filenames\n",
    "\n",
    "        image_paths = [self.corpus_dirpath+f\"{x}/{x}.png\" for x in self.corpus_list] # list of image paths\n",
    "        text_paths = [self.corpus_dirpath+f\"{x}/{x}.semantic\" for x in self.corpus_list]\n",
    "\n",
    "        image_texts = [] # list of strings\n",
    "\n",
    "        for path in text_paths:\n",
    "            with open(path, \"r\") as file:\n",
    "                image_texts.append(file.readline().split())\n",
    "\n",
    "        image_texts = self.encode_seqs(image_texts)\n",
    "\n",
    "        max_label_len = max([len(seq) for seq in image_texts])\n",
    "        print(max_label_len)\n",
    "        padded_image_texts = pad_sequences(image_texts, maxlen=max_label_len, padding='post', value= self.vocabulary_size + 1)\n",
    "\n",
    "        # TODO: do line 17 from the link : https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb\n",
    "\n",
    "        # TODO: Use self.word2int to encode each element of the lists in image_texts\n",
    "        # Input image_texts = [  ['clef','C-note'] , [.......], ....]\n",
    "        # Final image_texts = [  [5,3] , [.......], ....]\n",
    "        # if 'clef' == 5, 'C-note' == 3 in word2int dictionary\n",
    "        # Also add padding using the function below so length of each element is consistent\n",
    "\n",
    "        # from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        # padded_image_texts = list(map(encode_to_labels, image_texts))\n",
    "        # padded_image_texts[0]\n",
    "\n",
    "\n",
    "        train_image_paths = image_paths[ : int(len(image_paths) * val_split)]\n",
    "        train_image_texts = padded_image_texts[ : int(len(padded_image_texts) * val_split)]\n",
    "\n",
    "        val_image_paths = image_paths[int(len(image_paths) * val_split) : ]\n",
    "        val_image_texts = padded_image_texts[int(len(padded_image_texts) * val_split) : ]\n",
    "\n",
    "        return {\"train\":{\"images\":train_image_paths, \"text\": train_image_texts}, \"val\":{\"images\":val_image_paths, \"text\": val_image_texts}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "data_obj = PriMuS_Data(corpus_dirpath, corpus_filepath, dictionary_path)\n",
    "lst = data_obj.trim_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to desired dimensions with unknown width\n",
    "def resize_no_width(image, height):\n",
    "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
    "    sample_img = cv2.resize(image, (width, height))\n",
    "    return sample_img\n",
    "\n",
    "\n",
    "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
    "def process_single_sample(img_path, label,max_width = 30):\n",
    "    params = default_model_params(img_height,primus.vocabulary_size)\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img,[params['img_height'], max_width])\n",
    "    # TODO: Get height from params, calculate width from max of all images\n",
    "    return {\"image\": img, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "(None, 8, 125, 64)\n",
      "Model: \"OMR\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 64, 2003, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv_1 (Conv2D)                (None, 32, 1002, 32  32768       ['image[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_1_bn (BatchNormalization)  (None, 32, 1002, 32  128        ['conv_1[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_1_relu (ReLU)             (None, 32, 1002, 32  0           ['conv_1_bn[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max1 (MaxPooling2D)            (None, 16, 501, 32)  0           ['conv_1_relu[0][0]']            \n",
      "                                                                                                  \n",
      " conv_2 (Conv2D)                (None, 16, 251, 64)  8388608     ['max1[0][0]']                   \n",
      "                                                                                                  \n",
      " conv_2_bn (BatchNormalization)  (None, 16, 251, 64)  256        ['conv_2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_2_relu (ReLU)             (None, 16, 251, 64)  0           ['conv_2_bn[0][0]']              \n",
      "                                                                                                  \n",
      " max2 (MaxPooling2D)            (None, 8, 125, 64)   0           ['conv_2_relu[0][0]']            \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 125, 512)     0           ['max2[0][0]']                   \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 125, 256)    493056      ['reshape[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 125, 256)     0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 125, 256)    296448      ['dropout[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " label (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 125, 1782)    457974      ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " ctc_loss (CTCLayer)            (None, 125, 1782)    0           ['label[0][0]',                  \n",
      "                                                                  'dense2[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,669,238\n",
      "Trainable params: 9,669,046\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "image_widths = [cv2.imread(img).shape[1] for img in lst[\"train\"][\"images\"]]\n",
    "max_image_width = max(image_widths)\n",
    "print(max_image_width)\n",
    "# Parameterization\n",
    "img_height = 64\n",
    "params = default_model_params(img_height,primus.vocabulary_size)\n",
    "max_epochs = 100\n",
    "dropout = 0.5\n",
    "# Model\n",
    "model = buildModel(params,input_width = max_image_width)\n",
    "model.summary()\n",
    "\n",
    "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
    "def process_single_sample(img_path, label):\n",
    "    params = default_model_params(img_height,primus.vocabulary_size)\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img,[params['img_height'], max_image_width])\n",
    "    # TODO: Get height from params, calculate width from max of all images\n",
    "    return {\"image\": img, \"label\": label}\n",
    "# train_dataset = train_dataset.map(process_single_sample(max_width = max_image_width))\n",
    "\n",
    "# processed_images = [process_single_sample(img,label,max_image_width) for img,label in zip(lst[\"train\"][\"images\"], lst[\"train\"][\"text\"])]\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
    "\n",
    "# train_dataset = (\n",
    "#     train_dataset.map(\n",
    "#         process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "#     )\n",
    "#     .batch(batch_size)\n",
    "#     .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "# )\n",
    "\n",
    "# validation_dataset = tf.data.Dataset.from_tensor_slices((lst['val']['images'], lst['val']['text']))\n",
    "# validation_dataset = (\n",
    "#     validation_dataset.map(\n",
    "#         process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "#     )\n",
    "#     .batch(batch_size)\n",
    "#     .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "# )\n",
    "def tfdata_generator_train(batch_size= 16):\n",
    "  '''Construct a data generator using `tf.Dataset`. '''\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
    "  train_dataset = train_dataset.map(process_single_sample)\n",
    "  train_dataset = train_dataset.batch(batch_size)\n",
    "  # train_dataset = train_dataset.repeat()\n",
    "  train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  return train_dataset\n",
    "\n",
    "def tfdata_generator_val(batch_size= 16):\n",
    "  '''Construct a data generator using `tf.Dataset`. '''\n",
    "  val_dataset = tf.data.Dataset.from_tensor_slices((lst['val']['images'], lst['val']['text']))\n",
    "  val_dataset = val_dataset.map(process_single_sample)\n",
    "  val_dataset = val_dataset.batch(batch_size)\n",
    "  # val_dataset = val_dataset.repeat()\n",
    "  val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  return val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tfdata_generator_train()\n",
    "validation_dataset = tfdata_generator_val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'image': TensorSpec(shape=(None, 64, 2003, 1), dtype=tf.float32, name=None), 'label': TensorSpec(shape=(None, 56), dtype=tf.int32, name=None)}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='../models/model.{epoch:02d}-{val_loss:.2f}.ckpt', save_weights_only=True, verbose= 1),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='../data/logs'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs.\n",
    "epochs = 1\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(\n",
    "#     train_dataset,\n",
    "#     # tf.compat.v1.data.make_one_shot_iterator(train_dataset).get_next(),\n",
    "#     # validation_data = tf.compat.v1.data.make_one_shot_iterator(validation_dataset).get_next(),\n",
    "#     validation_data = validation_dataset,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=my_callbacks\n",
    "# )\n",
    "# model.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../models/model_1epoch.hdf5')\n",
    "# This\n",
    "# model.save_weights(\"../models/model_1epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x22ad27446d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"../models/model_1epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CTCLayer.__init__() got an unexpected keyword argument 'trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steli\\DeepLearning\\Project\\dl_omr\\notebooks\\ctc_model_fit_New_perspective.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000018?line=0'>1</a>\u001b[0m \u001b[39m# new_model = tf.keras.models.load_model('../models/model.01-105.60.h5', custom_objects={'CTCLayer': CTCLayer})\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000018?line=1'>2</a>\u001b[0m new_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39m../models/model_1epoch.hdf5\u001b[39;49m\u001b[39m'\u001b[39;49m, custom_objects\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mCTCLayer\u001b[39;49m\u001b[39m'\u001b[39;49m: CTCLayer})\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:796\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=779'>780</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=780'>781</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_config\u001b[39m(\u001b[39mcls\u001b[39m, config):\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=781'>782</a>\u001b[0m   \u001b[39m\"\"\"Creates a layer from its config.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=782'>783</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=783'>784</a>\u001b[0m \u001b[39m  This method is the reverse of `get_config`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=793'>794</a>\u001b[0m \u001b[39m      A layer instance.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=794'>795</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=795'>796</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "\u001b[1;31mTypeError\u001b[0m: CTCLayer.__init__() got an unexpected keyword argument 'trainable'"
     ]
    }
   ],
   "source": [
    "# new_model = tf.keras.models.load_model('../models/model.01-105.60.h5', custom_objects={'CTCLayer': CTCLayer})\n",
    "# new_model = tf.keras.models.load_model('../models/model_1epoch.hdf5', custom_objects={'CTCLayer': CTCLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CTCLayer.__init__() got an unexpected keyword argument 'trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steli\\DeepLearning\\Project\\dl_omr\\notebooks\\ctc_model_fit_New_perspective.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39m# Retrieve the config\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit_New_perspective.ipynb#ch0000016?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39m../models/model.01-105.60.h5\u001b[39;49m\u001b[39m'\u001b[39;49m, custom_objects\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mCTCLayer\u001b[39;49m\u001b[39m'\u001b[39;49m: CTCLayer})\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:796\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=779'>780</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=780'>781</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_config\u001b[39m(\u001b[39mcls\u001b[39m, config):\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=781'>782</a>\u001b[0m   \u001b[39m\"\"\"Creates a layer from its config.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=782'>783</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=783'>784</a>\u001b[0m \u001b[39m  This method is the reverse of `get_config`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=793'>794</a>\u001b[0m \u001b[39m      A layer instance.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=794'>795</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/engine/base_layer.py?line=795'>796</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "\u001b[1;31mTypeError\u001b[0m: CTCLayer.__init__() got an unexpected keyword argument 'trainable'"
     ]
    }
   ],
   "source": [
    "# Loading Model\n",
    "# model = tf.keras.models.load_model('../models/model.01-105.60.h5', custom_objects={'CTCLayer': CTCLayer})\n",
    "# Loading Weights"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9bbe03f6c4bbdf56f443191d16980388af1b731aba7be9951b0256cee325895"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
