{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D, Layer\n",
    "from keras.layers import Input, Dense, Activation, LeakyReLU, Permute, Bidirectional, CuDNNLSTM\n",
    "from keras.layers import Reshape, Lambda, BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from primus import CTC_PriMuS\n",
    "import ctc_utils\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from primus import CTC_PriMuS\n",
    "import ctc_utils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_model_params(img_height, vocabulary_size):\n",
    "    params = dict()\n",
    "    params['img_height'] = img_height\n",
    "    params['img_width'] = None\n",
    "    params['batch_size'] = 128\n",
    "    params['img_channels'] = 1\n",
    "    params['conv_blocks'] = 4\n",
    "    params['conv_filter_n'] = [32, 64, 128, 256]\n",
    "    params['conv_filter_size'] = [ [3,3], [3,3], [3,3], [3,3] ]\n",
    "    params['conv_pooling_size'] = [ [2,2], [2,2], [2,2], [2,2] ]\n",
    "    params['rnn_units'] = 128\n",
    "    params['rnn_layers'] = 2\n",
    "    params['vocabulary_size'] = vocabulary_size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = K.ctc_batch_cost\n",
    "        # self.loss_fn = K.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") # 16\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        print(\"Y_pred shape:  \",y_pred.shape)\n",
    "        print(\"Y_true shape:  \",y_true.shape)\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ctc_crnn(params,width_rem = 128):\n",
    "    input_shape = (params['img_height'],params['img_width'], params['img_channels'])\n",
    "\n",
    "    inputs = Input(name='image', shape=input_shape, dtype='float32')\n",
    "\n",
    "    labels = Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "\n",
    "    width_reduction = 1\n",
    "    height_reduction = 1\n",
    "\n",
    "    #conv2d layer\n",
    "    for i in range(params['conv_blocks']):\n",
    "            inner = Conv2D(params['conv_filter_n'][i], params['conv_filter_size'][i], padding='same', name='conv'+ str(i+1), kernel_initializer='he_normal')(inputs if i == 0 else inner)\n",
    "            inner = BatchNormalization()(inner)\n",
    "            inner = LeakyReLU(0.2)(inner)\n",
    "            inner = MaxPooling2D(pool_size=params['conv_pooling_size'][i], strides = params['conv_pooling_size'][i], name='max' + str(i+1))(inner)\n",
    "\n",
    "            width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
    "            height_reduction = height_reduction * params['conv_pooling_size'][i][0]\n",
    "            print(width_reduction)\n",
    "    \n",
    "    print(width_reduction)\n",
    "    features = K.permute_dimensions(inner, (2,0,3,1))\n",
    "    feature_dim = params['conv_filter_n'][-1] * (params['img_height'] / height_reduction)\n",
    "    feature_width = width_rem / width_reduction\n",
    "    print(\"Feature width:\",feature_width)\n",
    "    features = tf.reshape(features, tf.stack([tf.cast(feature_width,'int32'), 16, tf.cast(feature_dim,'int32')]))\n",
    "    \n",
    "    # RNN block\n",
    "    lstm_1 = CuDNNLSTM(params['rnn_units'], return_sequences=True, kernel_initializer='he_normal', name='lstm1')(features)  # (None, 32, 512)\n",
    "    lstm_1b = CuDNNLSTM(params['rnn_units'], return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm1_b')(features)\n",
    "    reversed_lstm_1b = Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_1b)\n",
    "\n",
    "    lstm1_merged = add([lstm_1, reversed_lstm_1b])  # (None, 32, 512)\n",
    "    lstm1_merged = BatchNormalization()(lstm1_merged)\n",
    "    \n",
    "    lstm_2 = CuDNNLSTM(params['rnn_units'], return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm1_merged)\n",
    "    lstm_2b = CuDNNLSTM(params['rnn_units'], return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm2_b')(lstm1_merged)\n",
    "    reversed_lstm_2b= Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_2b)\n",
    "\n",
    "    lstm2_merged = concatenate([lstm_2, reversed_lstm_2b])  # (None, 32, 1024)\n",
    "    lstm2_merged = BatchNormalization()(lstm2_merged)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    num_classes = params['vocabulary_size'] + 1\n",
    "    inner = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(lstm2_merged) #(None, 32, 63)\n",
    "    y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, y_pred)\n",
    "\n",
    "\n",
    "    return Model(inputs=[inputs, labels], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 9000 and validating with 1000\n"
     ]
    }
   ],
   "source": [
    "corpus_dirpath = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/primusCalvoRizoAppliedSciences2018/\"\n",
    "corpus_filepath = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/train.txt\"\n",
    "dictionary_path = \"C:/Users/steli/DeepLearning/Project/tf-end-to-end/Data/vocabulary_semantic.txt\"\n",
    "primus = CTC_PriMuS(corpus_dirpath,corpus_filepath,dictionary_path, True, val_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "image_paths = []\n",
    "image_texts = []\n",
    "\n",
    "class PriMuS_Data:\n",
    "    def __init__(self, corpus_dirpath, corpus_filepath, dictionary_path):\n",
    "        self.corpus_dirpath = corpus_dirpath\n",
    "\n",
    "        # Corpus\n",
    "        corpus_file = open(corpus_filepath,'r')\n",
    "        self.corpus_list = corpus_file.read().splitlines()\n",
    "        corpus_file.close()\n",
    "\n",
    "        self.current_idx = 0\n",
    "\n",
    "        # Dictionary\n",
    "        self.word2int = {}\n",
    "        self.int2word = {}\n",
    "            \n",
    "        dict_file = open(dictionary_path,'r')\n",
    "        dict_list = dict_file.read().splitlines()\n",
    "        for word in dict_list:\n",
    "            if not word in self.word2int:\n",
    "                word_idx = len(self.word2int)\n",
    "                self.word2int[word] = word_idx\n",
    "                self.int2word[word_idx] = word\n",
    "\n",
    "        dict_file.close()\n",
    "\n",
    "        self.vocabulary_size = len(self.word2int)\n",
    "    \n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vocabulary_size\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.corpus_list\n",
    "    \n",
    "    def trim_ds(self):\n",
    "        val_split = 0.9\n",
    "        folder_path = self.corpus_dirpath\n",
    "        list_of_files = self.corpus_list #10,000 filenames\n",
    "\n",
    "        image_paths = [self.corpus_dirpath+f\"{x}/{x}.png\" for x in self.corpus_list] # list of image paths\n",
    "        text_paths = [self.corpus_dirpath+f\"{x}/{x}.semantic\" for x in self.corpus_list]\n",
    "\n",
    "        image_texts = [] # list of strings\n",
    "\n",
    "        for path in text_paths:\n",
    "            with open(path, \"r\") as file:\n",
    "                image_texts.append(file.readline())\n",
    "        \n",
    "\n",
    "        # TODO: do line 17 from the link : https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb\n",
    "\n",
    "        # TODO: Use self.word2int to encode each element of the lists in image_texts\n",
    "        # Input image_texts = [  ['clef','C-note'] , [.......], ....]\n",
    "        # Final image_texts = [  [5,3] , [.......], ....]\n",
    "        # if 'clef' == 5, 'C-note' == 3 in word2int dictionary\n",
    "        # Also add padding using the function below so length of each element is consistent\n",
    "\n",
    "        # from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "        # def encode_to_labels(txt):\n",
    "        #     # encoding each output word into digits\n",
    "        #     dig_lst = []\n",
    "        #     for index, char in enumerate(txt):\n",
    "        #         try:\n",
    "        #             dig_lst.append(char_list.index(char))\n",
    "        #         except:\n",
    "        #             print(char)\n",
    "        #     return pad_sequences([dig_lst], maxlen=max_label_len, padding='post', value=len(char_list))[0]\n",
    "\n",
    "        # padded_image_texts = list(map(encode_to_labels, image_texts))\n",
    "        # padded_image_texts[0]\n",
    "\n",
    "\n",
    "        train_image_paths = image_paths[ : int(len(image_paths) * val_split)]\n",
    "        train_image_texts = image_texts[ : int(len(image_texts) * val_split)]\n",
    "\n",
    "        val_image_paths = image_paths[int(len(image_paths) * val_split) : ]\n",
    "        val_image_texts = image_texts[int(len(image_texts) * val_split) : ]\n",
    "\n",
    "        return {\"train\":{\"images\":train_image_paths, \"text\": train_image_texts}, \"val\":{\"images\":val_image_paths, \"text\": val_image_texts}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do line 17 from the link : https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = PriMuS_Data(corpus_dirpath, corpus_filepath, dictionary_path)\n",
    "lst = data_obj.trim_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781\n"
     ]
    }
   ],
   "source": [
    "print(data_obj.vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to desired dimensions with unknown width\n",
    "def resize_no_width(image, height):\n",
    "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
    "    sample_img = cv2.resize(image, (width, height))\n",
    "    return sample_img\n",
    "\n",
    "\n",
    "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
    "def process_single_sample(img_path, label,max_width = 30):\n",
    "    params = default_model_params(img_height,primus.vocabulary_size)\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img,[params['img_height'], max_width])\n",
    "    # TODO: Get height from params, calculate width from max of all images\n",
    "    return {\"image\": img, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "image_widths = [cv2.imread(img).shape[1] for img in lst[\"train\"][\"images\"]]\n",
    "max_image_width = max(image_widths)\n",
    "print(max_image_width)\n",
    "\n",
    "\n",
    "# Pre-process a single image sample(resize/normalize/max_width resize)\n",
    "def process_single_sample(img_path, label):\n",
    "    params = default_model_params(img_height,primus.vocabulary_size)\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img,[params['img_height'], max_image_width])\n",
    "    # TODO: Get height from params, calculate width from max of all images\n",
    "    return {\"image\": img, \"label\": label}\n",
    "# train_dataset = train_dataset.map(process_single_sample(max_width = max_image_width))\n",
    "\n",
    "# processed_images = [process_single_sample(img,label,max_image_width) for img,label in zip(lst[\"train\"][\"images\"], lst[\"train\"][\"text\"])]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((lst['train']['images'], lst['train']['text']))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "16\n",
      "Feature width: 125.1875\n",
      "Y_pred shape:   (125, 16, 1782)\n",
      "Y_true shape:   (None, None)\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 128, None,   0           []                               \n",
      "                                1)]                                                               \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 128, None, 3  320         ['image[0][0]']                  \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 128, None, 3  128        ['conv1[0][0]']                  \n",
      " ormalization)                  2)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 128, None, 3  0           ['batch_normalization_42[0][0]'] \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " max1 (MaxPooling2D)            (None, 64, None, 32  0           ['leaky_re_lu_28[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 64, None, 64  18496       ['max1[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 64, None, 64  256        ['conv2[0][0]']                  \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 64, None, 64  0           ['batch_normalization_43[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max2 (MaxPooling2D)            (None, 32, None, 64  0           ['leaky_re_lu_29[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3 (Conv2D)                 (None, 32, None, 12  73856       ['max2[0][0]']                   \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 32, None, 12  512        ['conv3[0][0]']                  \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_30 (LeakyReLU)     (None, 32, None, 12  0           ['batch_normalization_44[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max3 (MaxPooling2D)            (None, 16, None, 12  0           ['leaky_re_lu_30[0][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv4 (Conv2D)                 (None, 16, None, 25  295168      ['max3[0][0]']                   \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 16, None, 25  1024       ['conv4[0][0]']                  \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_31 (LeakyReLU)     (None, 16, None, 25  0           ['batch_normalization_45[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " max4 (MaxPooling2D)            (None, 8, None, 256  0           ['leaky_re_lu_31[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_7 (TFOp  (None, None, 256, 8  0          ['max4[0][0]']                   \n",
      " Lambda)                        )                                                                 \n",
      "                                                                                                  \n",
      " tf.reshape_7 (TFOpLambda)      (125, 16, 2048)      0           ['tf.compat.v1.transpose_7[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " lstm1_b (CuDNNLSTM)            (125, 16, 128)       1115136     ['tf.reshape_7[0][0]']           \n",
      "                                                                                                  \n",
      " lstm1 (CuDNNLSTM)              (125, 16, 128)       1115136     ['tf.reshape_7[0][0]']           \n",
      "                                                                                                  \n",
      " lambda_14 (Lambda)             (125, 16, 128)       0           ['lstm1_b[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (125, 16, 128)       0           ['lstm1[0][0]',                  \n",
      "                                                                  'lambda_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (125, 16, 128)      512         ['add_7[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm2_b (CuDNNLSTM)            (125, 16, 128)       132096      ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " lstm2 (CuDNNLSTM)              (125, 16, 128)       132096      ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_15 (Lambda)             (125, 16, 128)       0           ['lstm2_b[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (125, 16, 256)       0           ['lstm2[0][0]',                  \n",
      "                                                                  'lambda_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (125, 16, 256)      1024        ['concatenate_7[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (125, 16, 1782)      457974      ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " label (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " softmax (Activation)           (125, 16, 1782)      0           ['dense2[0][0]']                 \n",
      "                                                                                                  \n",
      " ctc_loss (CTCLayer)            (125, 16, 1782)      0           ['label[0][0]',                  \n",
      "                                                                  'softmax[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,343,734\n",
      "Trainable params: 3,342,006\n",
      "Non-trainable params: 1,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parameterization\n",
    "img_height = 128\n",
    "params = default_model_params(img_height,primus.vocabulary_size)\n",
    "max_epochs = 100\n",
    "dropout = 0.5\n",
    "# Model\n",
    "model = ctc_crnn(params, width_rem= max_image_width)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.Series(image_widths)\n",
    "plt.hist(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/master/CRNN_CTC_wandb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='label'), name='label', description=\"created by layer 'label'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"ctc_loss\" (type CTCLayer).\n    \n    in user code:\n    \n        File \"C:\\Users\\steli\\AppData\\Local\\Temp\\ipykernel_8712\\538555551.py\", line 13, in call  *\n            label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n    \n        ValueError: slice index 1 of dimension 0 out of bounds. for '{{node model_7/ctc_loss/strided_slice_2}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](model_7/ctc_loss/Shape_2, model_7/ctc_loss/strided_slice_2/stack, model_7/ctc_loss/strided_slice_2/stack_1, model_7/ctc_loss/strided_slice_2/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n    \n    \n    Call arguments received:\n      • y_true=tf.Tensor(shape=(None,), dtype=float32)\n      • y_pred=tf.Tensor(shape=(125, 16, 1782), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steli\\DeepLearning\\Project\\dl_omr\\notebooks\\ctc_model_fit.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer \u001b[39m=\u001b[39m optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39m# checkpoint = ModelCheckpoint(filepath=file_path, \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=6'>7</a>\u001b[0m \u001b[39m#                                 monitor='val_loss', \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39m#                                 verbose=1, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=15'>16</a>\u001b[0m \u001b[39m#                       PlotPredictions(frequency=1),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=16'>17</a>\u001b[0m \u001b[39m#                       EarlyStopping(patience=3, verbose=1)]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=18'>19</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataset, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=19'>20</a>\u001b[0m                     epochs \u001b[39m=\u001b[39;49m max_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=20'>21</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=21'>22</a>\u001b[0m                     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=22'>23</a>\u001b[0m                     \u001b[39m# callbacks = callbacks_list,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/steli/DeepLearning/Project/dl_omr/notebooks/ctc_model_fit.ipynb#ch0000013?line=23'>24</a>\u001b[0m                     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/steli/.conda/envs/tf-gpu/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\steli\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"ctc_loss\" (type CTCLayer).\n    \n    in user code:\n    \n        File \"C:\\Users\\steli\\AppData\\Local\\Temp\\ipykernel_8712\\538555551.py\", line 13, in call  *\n            label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n    \n        ValueError: slice index 1 of dimension 0 out of bounds. for '{{node model_7/ctc_loss/strided_slice_2}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](model_7/ctc_loss/Shape_2, model_7/ctc_loss/strided_slice_2/stack, model_7/ctc_loss/strided_slice_2/stack_1, model_7/ctc_loss/strided_slice_2/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n    \n    \n    Call arguments received:\n      • y_true=tf.Tensor(shape=(None,), dtype=float32)\n      • y_pred=tf.Tensor(shape=(125, 16, 1782), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model.compile(optimizer = optimizer)\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath=file_path, \n",
    "#                                 monitor='val_loss', \n",
    "#                                 verbose=1, \n",
    "#                                 save_best_only=True, \n",
    "#                                 mode='min')\n",
    "\n",
    "#     callbacks_list = [checkpoint, \n",
    "#                       WandbCallback(monitor=\"val_loss\", \n",
    "#                                     mode=\"min\", \n",
    "#                                     log_weights=True),\n",
    "#                       PlotPredictions(frequency=1),\n",
    "#                       EarlyStopping(patience=3, verbose=1)]\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs = max_epochs,\n",
    "                    validation_data=validation_dataset,\n",
    "                    verbose = 1,\n",
    "                    # callbacks = callbacks_list,\n",
    "                    shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9bbe03f6c4bbdf56f443191d16980388af1b731aba7be9951b0256cee325895"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
